{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a28f30-f403-43e5-b5d3-16022c5c3700",
   "metadata": {},
   "source": [
    "# 작물 잎 사진으로 질병 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f826cdc-c57c-474c-b539-f8f97ae719bd",
   "metadata": {},
   "source": [
    "목표 : 전이학습 개념 확립 (CNN모델과 모델 학습 방식에서 어떤 차이점과 장점이 있는지)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3201ff7-a329-40c1-93c1-09251f4ee4de",
   "metadata": {},
   "source": [
    "데이터수:  40000\n",
    "\n",
    "각 클래스 명 : 작물의 종류, 질병 종류 (healthy - 해당 작물이 건강함)\n",
    "\n",
    "ex) potato -> potato early blight/ potato late bright / potato_healthy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b562e3f-33c8-40c6-8a93-8c01bf105633",
   "metadata": {},
   "source": [
    "두가지 모델을 구축후 성능을 비교평가.\n",
    "1. CNN 구조를 활용하여 가장 기본적 베이스라인 모델 구축\n",
    "2. 미리 학습되어있는 모델을 사용하는 transfer learning 기법을 호라용하여 모델을 학습시킨 후, 베이스라인 모델과 비교.\n",
    "\n",
    "두 가지 모델의 성능뿐 아니라 구조와 활용방법 비교\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f1741-3c8c-419d-a3f2-ac4de0eb88f0",
   "metadata": {},
   "source": [
    "데이터 셋 ( 학습 / 검증 / 테스트 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca1abe-e81f-408d-b2df-c22bf6229680",
   "metadata": {},
   "source": [
    "# 데이터로드 및 분할폴더생성( 학습 / 검증 / 테스트 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "795de48b-c206-4691-a118-220617ea4c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    " \n",
    "original_dataset_dir = './dataset'                 #원본데이터셋 위치한 경로 지정\n",
    "classes_list = os.listdir(original_dataset_dir)    #os.listdir() 해당 경로 하위에 있는 모든 폴더 목록을 가져오는 메서드. 이 경우 폴더 목록은 클래스의 목록에 해당하므로 이것을 Class_list로 저장\n",
    " \n",
    "base_dir = './splitted'                            #나눈 데이터를 저장할 폴더 생성\n",
    "os.mkdir(base_dir)\n",
    " \n",
    "train_dir = os.path.join(base_dir, 'train')        #분리 후 각 데이터를 저장할 하위폴더 train, val, test 생성\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'val')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "for cls in classes_list:     \n",
    "    os.mkdir(os.path.join(train_dir, cls))\n",
    "    os.mkdir(os.path.join(validation_dir, cls))\n",
    "    os.mkdir(os.path.join(test_dir, cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069515f6-9dc4-4b30-bf29-bb03ff720e09",
   "metadata": {},
   "source": [
    "# 분할폴더에 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72462d1f-9392-4197-8a64-fa5198866d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size( Strawberry___healthy ):  273\n",
      "Validation size( Strawberry___healthy ):  91\n",
      "Test size( Strawberry___healthy ):  91\n",
      "Train size( Grape___Black_rot ):  708\n",
      "Validation size( Grape___Black_rot ):  236\n",
      "Test size( Grape___Black_rot ):  236\n",
      "Train size( Potato___Early_blight ):  600\n",
      "Validation size( Potato___Early_blight ):  200\n",
      "Test size( Potato___Early_blight ):  200\n",
      "Train size( Cherry___Powdery_mildew ):  631\n",
      "Validation size( Cherry___Powdery_mildew ):  210\n",
      "Test size( Cherry___Powdery_mildew ):  210\n",
      "Train size( Tomato___Target_Spot ):  842\n",
      "Validation size( Tomato___Target_Spot ):  280\n",
      "Test size( Tomato___Target_Spot ):  280\n",
      "Train size( Peach___healthy ):  216\n",
      "Validation size( Peach___healthy ):  72\n",
      "Test size( Peach___healthy ):  72\n",
      "Train size( Potato___Late_blight ):  600\n",
      "Validation size( Potato___Late_blight ):  200\n",
      "Test size( Potato___Late_blight ):  200\n",
      "Train size( Tomato___Late_blight ):  1145\n",
      "Validation size( Tomato___Late_blight ):  381\n",
      "Test size( Tomato___Late_blight ):  381\n",
      "Train size( Tomato___Tomato_mosaic_virus ):  223\n",
      "Validation size( Tomato___Tomato_mosaic_virus ):  74\n",
      "Test size( Tomato___Tomato_mosaic_virus ):  74\n",
      "Train size( Pepper,_bell___healthy ):  886\n",
      "Validation size( Pepper,_bell___healthy ):  295\n",
      "Test size( Pepper,_bell___healthy ):  295\n",
      "Train size( Tomato___Leaf_Mold ):  571\n",
      "Validation size( Tomato___Leaf_Mold ):  190\n",
      "Test size( Tomato___Leaf_Mold ):  190\n",
      "Train size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  645\n",
      "Validation size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
      "Test size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
      "Train size( Apple___Cedar_apple_rust ):  165\n",
      "Validation size( Apple___Cedar_apple_rust ):  55\n",
      "Test size( Apple___Cedar_apple_rust ):  55\n",
      "Train size( Tomato___Bacterial_spot ):  1276\n",
      "Validation size( Tomato___Bacterial_spot ):  425\n",
      "Test size( Tomato___Bacterial_spot ):  425\n",
      "Train size( Grape___healthy ):  253\n",
      "Validation size( Grape___healthy ):  84\n",
      "Test size( Grape___healthy ):  84\n",
      "Train size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  307\n",
      "Validation size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
      "Test size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
      "Train size( Tomato___Early_blight ):  600\n",
      "Validation size( Tomato___Early_blight ):  200\n",
      "Test size( Tomato___Early_blight ):  200\n",
      "Train size( Grape___Esca_(Black_Measles) ):  829\n",
      "Validation size( Grape___Esca_(Black_Measles) ):  276\n",
      "Test size( Grape___Esca_(Black_Measles) ):  276\n",
      "Train size( Tomato___healthy ):  954\n",
      "Validation size( Tomato___healthy ):  318\n",
      "Test size( Tomato___healthy ):  318\n",
      "Train size( Corn___Northern_Leaf_Blight ):  591\n",
      "Validation size( Corn___Northern_Leaf_Blight ):  197\n",
      "Test size( Corn___Northern_Leaf_Blight ):  197\n",
      "Train size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  3214\n",
      "Validation size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n",
      "Test size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n",
      "Train size( Cherry___healthy ):  512\n",
      "Validation size( Cherry___healthy ):  170\n",
      "Test size( Cherry___healthy ):  170\n",
      "Train size( Apple___Apple_scab ):  378\n",
      "Validation size( Apple___Apple_scab ):  126\n",
      "Test size( Apple___Apple_scab ):  126\n",
      "Train size( Tomato___Spider_mites Two-spotted_spider_mite ):  1005\n",
      "Validation size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
      "Test size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
      "Train size( Corn___Common_rust ):  715\n",
      "Validation size( Corn___Common_rust ):  238\n",
      "Test size( Corn___Common_rust ):  238\n",
      "Train size( Peach___Bacterial_spot ):  1378\n",
      "Validation size( Peach___Bacterial_spot ):  459\n",
      "Test size( Peach___Bacterial_spot ):  459\n",
      "Train size( Pepper,_bell___Bacterial_spot ):  598\n",
      "Validation size( Pepper,_bell___Bacterial_spot ):  199\n",
      "Test size( Pepper,_bell___Bacterial_spot ):  199\n",
      "Train size( Tomato___Septoria_leaf_spot ):  1062\n",
      "Validation size( Tomato___Septoria_leaf_spot ):  354\n",
      "Test size( Tomato___Septoria_leaf_spot ):  354\n",
      "Train size( Corn___healthy ):  697\n",
      "Validation size( Corn___healthy ):  232\n",
      "Test size( Corn___healthy ):  232\n",
      "Train size( Apple___Black_rot ):  372\n",
      "Validation size( Apple___Black_rot ):  124\n",
      "Test size( Apple___Black_rot ):  124\n",
      "Train size( Apple___healthy ):  987\n",
      "Validation size( Apple___healthy ):  329\n",
      "Test size( Apple___healthy ):  329\n",
      "Train size( Strawberry___Leaf_scorch ):  665\n",
      "Validation size( Strawberry___Leaf_scorch ):  221\n",
      "Test size( Strawberry___Leaf_scorch ):  221\n",
      "Train size( Potato___healthy ):  91\n",
      "Validation size( Potato___healthy ):  30\n",
      "Test size( Potato___healthy ):  30\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    " \n",
    "for cls in classes_list:                                                                          #모든 클래스에 대한 작업 반복\n",
    "    path = os.path.join(original_dataset_dir, cls)\n",
    "    fnames = os.listdir(path)                                                                     #path 위치에 존재하는 모든 이미지파일의 목록을 변수 fnames에 저장\n",
    " \n",
    "    train_size = math.floor(len(fnames) * 0.6)\n",
    "    validation_size = math.floor(len(fnames) * 0.2)\n",
    "    test_size = math.floor(len(fnames) * 0.2)                                                     # 6:2:2 비율로 지정\n",
    "    \n",
    "    train_fnames = fnames[:train_size]                                                            #Train 데이터에 해당하는 파일의 이름을 train_fnames에 저장\n",
    "    print(\"Train size(\",cls,\"): \", len(train_fnames))\n",
    "\n",
    "    for fname in train_fnames:                                                                    \n",
    "        src = os.path.join(path, fname)                                                           #복사할 원본파일 경로 지정\n",
    "        dst = os.path.join(os.path.join(train_dir, cls), fname)                                   #복사한후 저장할 파일의 경로를 지정\n",
    "        shutil.copyfile(src, dst)                                                                 #src경로에 해당하는 파일을 dst 경로에 저장\n",
    "        \n",
    "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
    "    print(\"Validation size(\",cls,\"): \", len(validation_fnames))\n",
    "    for fname in validation_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "        \n",
    "    test_fnames = fnames[(train_size+validation_size):(validation_size + train_size +test_size)]\n",
    "\n",
    "    print(\"Test size(\",cls,\"): \", len(test_fnames))\n",
    "    for fname in test_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2134c8-8296-4e63-9942-9f2e4fe7420a",
   "metadata": {},
   "source": [
    "# 베이스라인모델(CNN) 학습을 위한 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c1161f1-1a45-4012-b774-4efc0760cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    " \n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "BATCH_SIZE = 256 \n",
    "EPOCH = 30 \n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder \n",
    " \n",
    "transform_base = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])  #transforms.compose()는 이미지 데이터의 전처리, augmentation등의 과정에서 주로사용\n",
    "                                                                                          #이미지 증강에는 대표적으로 좌우반전, 밝기조절, 이미지 임의 확대 등이 있음.\n",
    "                                                                                          #증강을 통해 이미지에 노이즈를 주어 더욱 강건한 모델 생성 가능\n",
    "                                                                                          #Resize()-이미지크기를 64x64로 조정.\n",
    "                                                                                          #ToTensor()-이미지를 텐서형태로 변환하고 모든값을 0~1사이로 정규화\n",
    "train_dataset = ImageFolder(root='./splitted/train', transform=transform_base)            #ImageFolder()-하나의클래스(종류)가 하나의폴더에 대응하는 데이터셋형태 데이터셋 불러올때\n",
    "                                                                                          #transform 옵션에는 데이터를 불러온 후 전처리 또는 증강할 방법을 지정.\n",
    "val_dataset = ImageFolder(root='./splitted/val', transform=transform_base)                #검증을 위한 데이터를 val 폴더에 접근하여 불러오고, 학습데이터와 동일하게 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f799adee-9958-4b60-95a0-6e1df4e915ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "#dataloader는 불러온 이미지 데이터를 주어진 조건에 따라 미니배치 단위로 분리함. 학습과정에 사용될 dataloader는 train_dataset을 이용하여 생성.\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "#검증과정에 사용될 dataloader는 val_dataset을 이용하여 생성."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf8b26-5182-400e-a7b4-672bc1d21e38",
   "metadata": {},
   "source": [
    "# 베이스라인 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1397b436-c913-43f3-a3dd-ac4f5fb86eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    " \n",
    "class Net(nn.Module): \n",
    "  \n",
    "    def __init__(self):                                               #모델에서 사용할 모든 layer 정의\n",
    "    \n",
    "        super(Net, self).__init__()                                   #nn.Module내에 있는 메서드 상속받아 사용\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)                   #(4) 첫번째 2D convolution layer 정의. 입력된 이미지데이터에서 2차원 conv연산을 하는 필터에 해당\n",
    "                                                                      #(입력채널수, 출력채널수, 커널크기)\n",
    "        self.pool = nn.MaxPool2d(2,2)                                 #(5)2차원 maxpooling을 실행하는 layer정의. (커널크기, stride)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)                  #(6)(입력채널수, 출력채널수, 커널크기)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)                  #(7)(입력채널수, 출력채널수, 커널크기)\n",
    "\n",
    "        self.fc1 = nn.Linear(4096, 512)                               #flatten이후에 사용될 첫번째 fully connected layer정의 (입력채널수는 flatten의 출력 채널수와 동일)\n",
    "        self.fc2 = nn.Linear(512, 33)                                 #(9)flatten이후에 사용될 두번째 fully connected layer정의 (마지막 레이어이므로 출력채널수는 분류클래스 수와 동일)\n",
    "    \n",
    "    def forward(self, x):                                             #모델이 학습데이터 입력받아 forward propagation을 실행시켜 output계산 과정 정의\n",
    "    \n",
    "        x = self.conv1(x)                                             #(11)4에서 정의한 conv1 layer를 이용해 convolution연산을 진행한 후 feature map을 생성\n",
    "        x = F.relu(x)                                                 #11에서 conv 연산을 통해 생성된 피처맵값에 비선형 활성 함수인 ReLU를 적용\n",
    "        x = self.pool(x)                                              #maxpooling 적용\n",
    "        x = F.dropout(x, p=0.25, training=self.training)              #maxpooling 결과값에 dropout적용, p는 dropout 비율을 의미함.\n",
    "                                                                      #p=0.25 이므로 25%의 노드를 dropout한다는 의미 training=self.trainiing - 학습모드와 검증모드일때 각각 달리 적용\n",
    "                                                                      #학습과정에선 일부노드 랜덤하게 제외, 평가과정에선 모든 노드 사용해야하기 때문\n",
    "        x = self.conv2(x)                                             #6에서 정의한 conv2 layer를 이용해 conv 연산을 진행 후 feature map 생성\n",
    "        x = F.relu(x)                                                 #7에서 정의한 conv3 layer를 이용해 conv 연산을 진행 후 feature map 생성\n",
    "        x = self.pool(x)                                              \n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = self.conv3(x)                                             #7에서 정의한 conv3 layer를 이용해 conv 연산을 진행 후 feature map 생성\n",
    "        x = F.relu(x) \n",
    "        x = self.pool(x) \n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = x.view(-1, 4096)                                          #(17)생성된 feature map을 1차원으로 펼치는 과정인 flatten 수행\n",
    "        x = self.fc1(x)                                               #17에서 flatten된 1차원 텐서를 8에서 정의한 fc1에 통과시킴\n",
    "        x = F.relu(x)                                                 \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)                                               #9에서 정의한 모델의 마지막 layer. 클래스 개수에 해당하는 33개의 출력값을 가짐\n",
    "\n",
    "        return F.log_softmax(x, dim=1)                                #마지막 레이어의 33개 결과값에 softmax()함수를 적용하여 데이터가 각 클래스에 속할 확률을 output값으로 출력\n",
    "\n",
    "model_base = Net().to(DEVICE)                                         #정의한 cnn모델 net()의 새로운 객체를 생성합니다. to(DEVICE)를 통해 모델을 현재 사용중인 장비에 할당\n",
    "optimizer = optim.Adam(model_base.parameters(), lr=0.001)             #옵티마이저는 아담으로 설정하고, learning rate는 0.001로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918423ff-bceb-48db-8488-7fb894e1d641",
   "metadata": {},
   "source": [
    "# 모델학습을 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9f6e84b-fd4d-4604-a05f-3a6c965f14f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()                                                    #입력받는 모델을 학습 모드로 설정\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):        #앞서 정의했던 train_loader에는 (data, target)형태가 미니 배치 단위로 묶여 있음.\n",
    "                                                                     #train_loader에 enumerate함수를 적용하여 batch_idx, (data, target)형태로 반복 가능한 객체 생성되어 for 실행\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)            #data와 target변수를 사용중인 장비에 할당\n",
    "        optimizer.zero_grad()                                        #이전 batch의 gradient값이 optimizer에 저장되어 있으므로 optimizer 초기화\n",
    "        output = model(data)                                         #데이터를 모델에 입력하여 output값 계산\n",
    "        loss = F.cross_entropy(output, target)                       #(6)모델에서 계산한 output값이 예측값과 target값 사이의 Loss를 계산. 분류문제에 적합한 cross entropy loss 사용\n",
    "        loss.backward()                                              #(7)6에서 계산한 loss값 바탕으로 back propagation을 통해 계산한 gradient값을 각 파라미터에 할당\n",
    "        optimizer.step()                                             #7에서 각 parameter에 할당된 gradient값을 이용해 모델의 parameter업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167ff38-04b9-4a58-b7ee-c99169f216cd",
   "metadata": {},
   "source": [
    "# 모델 평가를 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "396cf2b1-1228-4582-928d-262bb9b6acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):                                               \n",
    "    model.eval()                                                                #입력받는 모델을 평가 모드로 설정\n",
    "    test_loss = 0                                                               #미니배치별로 loss를 합산해서 저장할 변수인 test_loss를 선언하고 0으로 초기화\n",
    "    correct = 0                                                                 #올바르게 예측한 데이터의 수를 세는 변수인 correct를 선언하고 0으로 초기화\n",
    "    \n",
    "    with torch.no_grad():                                                       #모델을 평가하는 단계에서는 모델의 parameter를 업데이트 하지 않아야하므로 with torch.no_grad()로 파라미터 업데이트 중단\n",
    "        for data, target in test_loader:                                        #앞에서 학습한 것과 같이 train_loaderdpsms (data, target)형태가 미니 배치 단위로 묶임. for를 통해 데이터와 대응하는 label값에 접근\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)                   #data와 target 변수를 사용중인 장비로 할당\n",
    "            output = model(data)                                                #데이터를 모델에 입력하여 output값 계산\n",
    "            \n",
    "            test_loss += F.cross_entropy(output,target, reduction='sum').item() #모델에서 계산한 output값인 예측값과 target값 사이의 loss계산. 성능평가 과정에서도 cross entropy loss 함수를 사용\n",
    " \n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]                               #모델에 입력된 Test 데이터가 33개의 클래스에 속할 각각의 확률값이 output으로 출력됨. 이중 가장 높은 값을 가진 인덱스를 예측값으로 저장\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()               #target.view_as(pred)를 통해 target Tensor 구조를 pred Tensor와 같은 모양으로 정렬.\n",
    "                                                                                #view_as()메서드는 적용 대상 텐서를 메서트에 입력되는 텐서 모양대로 재정렬하는 함수.\n",
    "                                                                                #view()함수는 정렬하고 싶은 텐서의 모양을 숫자로 직접 지정해야하는 것에서 차이 발생.\n",
    "                                                                                #eq()메서드는 객체간 비교 연산자로, pred.eq(target.view_as(pred))는 pred와 target.view_as(pred)의 값이 일치하면 1, 아니면 0\n",
    "                    \n",
    "   \n",
    "    test_loss /= len(test_loader.dataset)                                       #모든 미니 배치에서 합한 loss값을 batch수로 나누어 미니배치마다 계산된 정확도값의 평균을 구함\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)                   #모든 미니 배치에서 합한 정확도 값을 batch수로 나누어 미니배치마다 계산된 정확도 값의 평균을 구함\n",
    "    return test_loss, test_accuracy                                             #측정한 testloss와 정확도 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efae3cd-1f28-4d29-89dd-3e884d673996",
   "metadata": {},
   "source": [
    "# 모델 학습 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4535e42-a54c-4e54-ba43-976d3422c293",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 1 ----------------\n",
      "train Loss: 1.7331, Accuracy: 48.42%\n",
      "val Loss: 1.7330, Accuracy: 48.32%\n",
      "Completed in 2m 23s\n",
      "-------------- epoch 2 ----------------\n",
      "train Loss: 1.0845, Accuracy: 67.31%\n",
      "val Loss: 1.1103, Accuracy: 66.85%\n",
      "Completed in 2m 23s\n",
      "-------------- epoch 3 ----------------\n",
      "train Loss: 0.8228, Accuracy: 74.05%\n",
      "val Loss: 0.8616, Accuracy: 72.52%\n",
      "Completed in 2m 23s\n",
      "-------------- epoch 4 ----------------\n",
      "train Loss: 0.5779, Accuracy: 82.52%\n",
      "val Loss: 0.6228, Accuracy: 81.14%\n",
      "Completed in 2m 24s\n",
      "-------------- epoch 5 ----------------\n",
      "train Loss: 0.4897, Accuracy: 84.93%\n",
      "val Loss: 0.5412, Accuracy: 83.39%\n",
      "Completed in 2m 24s\n",
      "-------------- epoch 6 ----------------\n",
      "train Loss: 0.4264, Accuracy: 87.39%\n",
      "val Loss: 0.4923, Accuracy: 84.52%\n",
      "Completed in 2m 24s\n",
      "-------------- epoch 7 ----------------\n",
      "train Loss: 0.3790, Accuracy: 88.57%\n",
      "val Loss: 0.4535, Accuracy: 86.39%\n",
      "Completed in 2m 24s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(best_model_wts)  \n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 27\u001b[0m base \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_baseline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCH\u001b[49m\u001b[43m)\u001b[49m  \t \u001b[38;5;66;03m#(16)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(base,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseline.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36mtrain_baseline\u001b[0;34m(model, train_loader, val_loader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m train(model, train_loader, optimizer)\n\u001b[1;32m     11\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m evaluate(model, train_loader) \n\u001b[0;32m---> 12\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_acc \u001b[38;5;241m>\u001b[39m best_acc: \n\u001b[1;32m     15\u001b[0m     best_acc \u001b[38;5;241m=\u001b[39m val_acc \n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m      4\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m   \n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m test_loader:  \n\u001b[1;32m      8\u001b[0m         data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(DEVICE), target\u001b[38;5;241m.\u001b[39mto(DEVICE)  \n\u001b[1;32m      9\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(data) \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1305\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1305\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \n\u001b[1;32m   1310\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1430\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1430\u001b[0m     \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues:\n\u001b[1;32m   1432\u001b[0m     q\u001b[38;5;241m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentinel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    " \n",
    "def train_baseline(model ,train_loader, val_loader, optimizer, num_epochs = 30):\n",
    "    best_acc = 0.0                                                                           #정확도가 가장 높은 모델의 정확도를 저장하는 변수 best_acc를 선언후 값을 0으로 초기화\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())                                       #정확도 가장 높은 모델을 저장할 변수 best_model_wts 선언\n",
    " \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        since = time.time()                                                                  #한 epoch당 소요 시간 측정위해 해당 epoch시작할 때 시각 저장\n",
    "        train(model, train_loader, optimizer)                                                #앞서 정의한 train()함수를 이용하여 모델 학습\n",
    "        train_loss, train_acc = evaluate(model, train_loader)                                #앞서 정의한 evaluate()함수를 이용하여 해당 epoch에서의 학습 loss와 정확도를 계산\n",
    "        val_loss, val_acc = evaluate(model, val_loader)                                      #앞서 정의한 evaluate()함수를 이용하여 해당 epoch에서의 검증 loss와 정확도를 계산\n",
    "        \n",
    "        if val_acc > best_acc:                                                               #현재 epoch의 검증 정확도가 최고 정확도보다 높다면 best_acc를 현재 Epoch의 검증 정확도로 업데이트\n",
    "            best_acc = val_acc \n",
    "            best_model_wts = copy.deepcopy(model.state_dict())                               #해당 모델을 best_model_wts에 저장\n",
    "        \n",
    "        time_elapsed = time.time() - since                                                   #한 epoch당 소요된 시간 계산 \n",
    "        print('-------------- epoch {} ----------------'.format(epoch))                      \n",
    "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))         #해당 Epoch의 학습 loss와 정확도를 출력\n",
    "        print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))               #해당 Epoch의 검증 loss와 정확도를 출력\n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))  #한 epoch당 소요된 시간을 출력\n",
    "    model.load_state_dict(best_model_wts)                                                    #최종적으로 정확도가 가장 높은 모델을 불러오고\n",
    "    return model                                                                             #불러온 모델 반환\n",
    " \n",
    "\n",
    "base = train_baseline(model_base, train_loader, val_loader, optimizer, EPOCH)  \t             #앞서 정의한 train_baseline()함수를 이용해 baseline모델 학습 \n",
    "torch.save(base,'baseline.pt')                                                               #학습 완료된 모델 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba5bbab-a28c-48b0-af42-31fcf1578ff5",
   "metadata": {},
   "source": [
    "# 전이학습을 위한 데이터 준비 transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f24e1-d047-46f5-aead-c4326d4e647a",
   "metadata": {},
   "source": [
    "높은성능의 이미지 분류 모델 구축을 위해 많은 양의 질 좋은 데이터 필요.\n",
    "\n",
    "but 대개 양질의 데이터 대량 구하기 힘듦.\n",
    "\n",
    "-> 대량 데이터셋으로 미리 학습된 모델 재활용 후 일부 조정하여 다른 주제의 이미지 분류 모델에 사용하면 효과적\n",
    "\n",
    "대량의 데이터셋으로 미리 학습된 모델 : pre trained model\n",
    "\n",
    "pre trained model을 조정하는 과정 : fine tuning\n",
    "\n",
    "위 기법을 통틀어서 'transfer learning'.\n",
    "\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5887f5ca-5d38-4b80-8e36-236850f7993f",
   "metadata": {},
   "source": [
    "일반적으로 pytorch는 torchvision.models 패키지에서 imagenet데이터를 학습해놓은\n",
    "\n",
    "alexnet, vgg, resnet, squeezenet, densenet, inception v3, googlenet,\n",
    "\n",
    "shufflenetv2, mobilenet v2, resnext, wide resnet, mnasnet 등 모델을 바로 불로 올 수 있도록 지원. \n",
    "\n",
    "torchvision.models 공식 문서 참조\n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1a648e-288b-48a5-89c2-9bcb6d92ab64",
   "metadata": {},
   "source": [
    "베이스라인 모델의 경우 학습 진행시 초기 parameter값은 랜덤.\n",
    "\n",
    "Transfer learning의 경우 미리 학습된 모델의 params 불러오고 학습 과정에서 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d5e8df-e497-481e-bccd-4a132866ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize([64,64]),                    #transforms.compose()는 이미지 데이터 전처리, 증강 등 과정에서 사용되는 메서드, 이미지크기 64x64로 조정\n",
    "        transforms.RandomHorizontalFlip(),             #이미지 증강에 해당 무작위로 좌우 반전 (괄호)안에 param p를 입력하여 반전되는 이미지의 비율 설정가능\n",
    "        ransforms.RandomVerticalFlip(),                #기본 디폴트값 0.5  #무작위 상하반전\n",
    "        transforms.RandomCrop(52),                     #이미지 일부 무작위로 잘라내어 52x52사이즈로 변경/ centercrop, fivecrop등 다양한 방법 존재\n",
    "        transforms.ToTensor(),                         #이미지 탠서 형태로 변환하고 모든 숫자를 0에서 1사이로 변경.\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],    #이미지가 탠서 전환된 이후 정규화 시행. 정규화 위해서는 평균과 표준편차 값 필요.\n",
    "                             [0.229, 0.224, 0.225]) ]),#normalize()메서드 내의 첫번째 대괄호 []는 각각 r, g, b채널 값에서 정규화 적용할 평균 값 의미\n",
    "                                                       #두번째 [] 표준편차 값 의미. 사용된 평균값과 표준편차값은 pre trained model의 학습에 사용된 ImageNet 데이터 값임.\n",
    "                                                       #입력 데이터 정규화는 모델을 최적화하고 local minimum에 빠지는 것을 방지하는데 도움이 됩니다.\n",
    "    \n",
    "    'val': transforms.Compose([transforms.Resize([64,64]),  # 검증에 사용되는 데이터에는 학습에 적용했던 전처리에서 증각에 해당하는 부분 제외한 나머지부분 동일 적용\n",
    "        transforms.RandomCrop(52), transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
    "}\n",
    "\n",
    "data_dir = './splitted'   #학습데이터와 검증데이터를 불러올 폴더 경로 설정\n",
    "image_datasets = {x: ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x]) for x in ['train', 'val']}\n",
    "#ImageFolder()메서드는 데이터셋 불러오는 메서드. root 옵션에 데이터를 불러올 경로 설정. transform 옵션에는 데이터 불러온 후 전처리 또는 증강 방법 지정.\n",
    "#앞서 정의한 transfrom_base로 옵션 지정. 훈련 검증과정에서 각각의 과정에 맞는 데이터를 편리하게 불러오고자 딕셔너리 형태로 구성.\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']} \n",
    "#Dataloader는 불러온 이미지 데이터를 주어진 조건에 따라 미니 배치 단위로 분리하는 역할 수행함. image_datasets를 이용하여 생성하며,\n",
    "#셔플을 해야 모델 학습시 label정보 순서 기억하는 것 방지. 훈련 검증과정에서 각각 단계맞는 데이터 편리하게 불러오기 위해 딕셔너리로 구성\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "#활용을 위해 학습데이터와 검증 데이터의 총 개수 각각저장. 훈련 검증과정에서 각각 단계에 맞는 데이터 편리하게 불러오고자 딕셔너리로 저장\n",
    "\n",
    "class_names = image_datasets['train'].classes #이후 활용을 위해 33개의 클래스 이름 목록을 저장."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516abb45-e0c0-4975-bec6-5c475a6d7499",
   "metadata": {},
   "source": [
    "# pre-Trained model 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae763bca-22c2-4bc7-ab14-b0440f1142af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    " \n",
    "resnet = models.resnet50(pretrained=True)  #true로 설정시 미리 학습된 모델의 params값을 그대로 가져옴. false -> 구조만 가져오고 param랜덤\n",
    "num_ftrs = resnet.fc.in_features           #데이터 33개 클래스로 분류하므로 모델 마지막 레이어 출력 채널 수 33개여야함. 하지만 resnet50은 출력채널수 33개아님.\n",
    "                                           #따라서 마지막 fully connected layer 대신 33개의 새로운 layer추가 필요하여 마지막 layer의 입력 채널 수 저장\n",
    "                                           #in_features는 해당 layer의 입력 채널 수를 의미.\n",
    "resnet.fc = nn.Linear(num_ftrs, 33)        #불러온 모델의 마지막 fully connected layer를 새로운 layer로 교체. 입력 채널수는 기존 layer와 동일하고 출력은 33으로 설정\n",
    "resnet = resnet.to(DEVICE)                 #모델을 현재 사용중인 장비에 할당\n",
    " \n",
    "criterion = nn.CrossEntropyLoss()          #모델 학습시 사용되는 loss함수 지정 변수. 베이스라인 모델과 동일하게 cross entropy loss 사용\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=0.001)\n",
    "# 앞서 학습한 베이스라인 모델에서는 모든 파라미터 업데이트 했지만, 이 모델에선 설정한 일부 레이어의 파라미터만을 업데이트해야함.\n",
    "# ->따라서 filter()와 lambda표현식을 사용하여 requires_grad = True로 설정된 layer의 params에만 적용.\n",
    " \n",
    "from torch.optim import lr_scheduler \n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#StepLR메서드는 epoch에 따라 learning rate를 변경하는 역할. 7 epoch마다 0.1씩 곱해 learning rate를 감소시킨다는 의미.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5006920e-66ad-4a10-bc27-f77d9251ed66",
   "metadata": {},
   "source": [
    "# pre-Trained model의 일부 layer freeze하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ccfa3-e367-42f9-b20e-fdaec0e7f83f",
   "metadata": {},
   "source": [
    "pre trained model을 fine tuning 하는 과정에서 일부 layer는 학습 시 업데이트 안되게 하고 일부 layer는 업데이트 되도록 설정.\n",
    "\n",
    "일부 layer 업데이트 안되게 고정하는 것 : 'layer를 freeze한다'\n",
    "\n",
    "이미지 분류모델은 먼저 Lowlevel Feature 학습(모서리 같이 작은 지역패턴 학습) 후 -> Highlevel feature 학습(low level feature로 구성된 더 큰 패턴)\n",
    "\n",
    "다른 종류의 이미지라도 낮은 수준의 특징은 상대적으로 서로 비슷할 가능성이 큼.\n",
    "\n",
    "-> fine tuning 과정에서 마지막 layer인 분류기와 가까운 layer부터 원하는만큼 학습 과정에서 업데이트함.\n",
    "\n",
    "이때 freeze하는 layer 수는 데이터셋 크기와 pretrained model에 사용된 데이터셋과의 유사성을 고려하여 결정.\n",
    "\n",
    "4가지 case\n",
    "\n",
    "(자신이 가진 데이터셋 크기와 데이터셋이 pre-trainedmodel의 학습에 사용된 데이터와 얼마나 유사한지에 따라 4가지로 구분\n",
    "\n",
    "모델 구조는 크게 이미지의 feature를 학습하는 convolutional base 부분과 이미지를 분류하는 classifier부분으로 나뉨.\n",
    "\n",
    "분류하는 이미지가 다를경우 classifier 부분을 변경해야함.\n",
    "\n",
    "1. - 자신이 가진 데이터 수도 많고, 원래 학습에 이용된 데이터와의 유사도가 높을 경우. -> 일부 레이어만 freeze\n",
    "\n",
    "=> 데이터의 유사도가 높기 때문에 상대적으로 적은 수의 layer만 업데이트 해도 높은 성능 낼 수 있음.\n",
    "\n",
    "2. - 가진 데이터 수는 많지만, 원래 학습에 이용된 데이터와의 유사도 낮음 -> 전체 레이어 unfreeze\n",
    "\n",
    "=> 모든 레이어가 학습과정에서 업데이트 되지만, 모델의 효과적인 구조는 그대로 유지되고 불러온 파라미터값에서 조금씩 업데이트하며 조정하는 점이 처음부터 모델 구성하는 방법과 차이임.\n",
    "\n",
    "3. - 자신이 가진 데이터 적고, 유사도도 낮음 -> 일부 레이어 freeze\n",
    "\n",
    "=> 2의 경우 모델의 크기에 비해 데이터 수가 충분하여 전체 layer를 재학습해도 과적합 위험이 상대적으로 낮음. 3사분면의 경우 데이터 충분하지 않으므로 일부 layer만 업데이트 하는게 효과적.\n",
    "\n",
    "4. - 데이터 수는 적지만, 유사도는 높음. -> convolutional base부분 전체를 freeze하고 classifier부분만 변경\n",
    "\n",
    "=> 데이터 유사도가 높으므로 pre trained model이 학습한 많은 feature들을 그대로 사용할 수 있지만, 일부 layer를 재학습하기엔 과적합 위험이 상대적으로 크기 때문에 conv base 전체 freeze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4522e130-c9f7-43ee-85de-22c68f36c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = 0                                    #해당 레이어가 몇번째 레이어이진 나타내는 변수 ct 선언\n",
    "for child in resnet.children():           #children() - 자식 모듈을 반복 가능한 객체로 반환하는 메서드, resnet.children()- 생성한 resnet 모델의 모든 layer정보를 담고있음.\n",
    "    ct += 1                               #다음 레이어를 지칭하도록 for문 반복마다 1씩 증가\n",
    "    if ct < 6:                            #미리 학습된 모델의 일부 레이어만 업데이트하도록 파라미터 업데이트를 진행하지 않을 상위 레이어의 requires_grad = false로 지정\n",
    "        for param in child.parameters():  #child.parameter는 각 layer의 parameter tensor를 의미. 각 Tensor에는 requires_grad옵션이 있고 기본값 True\n",
    "            param.requires_grad = False   #layer의 번호가 6보다 작을 땐, parameter 업데이트 X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f09776-33ad-4984-a519-81f4e6de0537",
   "metadata": {},
   "source": [
    "# 전이학습 모델 학습과 검증을 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad61a5-fb16-4aa5-89e9-64aaf1bb3e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())                         #정확도가 가장 높은 모델 저장할 변수\n",
    "    best_acc = 0.0                                                             #정확도 가장 높은 모델의 정확도 저장할 변수\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('-------------- epoch {} ----------------'.format(epoch+1))      #현재 진행중인 epoch 출력\n",
    "        since = time.time()                                                    #한 epoch당 소요 시간 측정 위해 epoch 시작 시간 저장\n",
    "\n",
    "        for phase in ['train', 'val']:                                         #한 epoch는 각각 학습과 검증단계 지님. for통해 한 epoch마다 학습모드와 검증 각각 실행 \n",
    "            if phase == 'train':                                               #상황에 적합하게 모델을 학습모드로 설정\n",
    "                model.train() \n",
    "            else:                                                              #상황에 적합하게 모델을 검증모드로 설정\n",
    "                model.eval()     \n",
    " \n",
    "            running_loss = 0.0                                                 #모든 데이터의 loss를 합산하여 저장할 변수인 running_loss 선언\n",
    "            running_corrects = 0                                               #올바르게 예측한 경우의수를 세는 변수 선언\n",
    " \n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:                          #모델의 현재 모드에 해당하는 dataloader에서 데이터 입력받음\n",
    "                inputs = inputs.to(DEVICE)                                     #데이터와 label을 현재 사용중인 장비에 할당\n",
    "                labels = labels.to(DEVICE)  \n",
    "                \n",
    "                optimizer.zero_grad()                                          \n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):                 #학습단계에서만 모델의 gradient를 업데이트하고 검증단계에서는 업데이트 하지 않아야함. set_grad_enabled이용\n",
    "                    outputs = model(inputs)                                    #데이터를 모델에 입력하여 output값 계산\n",
    "                    _, preds = torch.max(outputs, 1)                           #(15)모델에 입력된 test데이터가 33개 클래스에 속할 각각 확률값이 output으로 출력됌. 가장 높은 값 지닌 인덱스 예측값으로 저장\n",
    "                    loss = criterion(outputs, labels)                          #(16)모델에서 계산한 output값인 예측값과 target값 사이의 Loss를 계산. 입력받는 Loss함수 이용하여 Loss 계산.\n",
    "    \n",
    "                    if phase == 'train':                                       #모델이 현재 학습 모드인 경우, 16에서 계산한 loss 바탕으로 back propagation을 통해 계산한 gradient값을 각 params에 할당후, 모델 params 업데이트\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    " \n",
    "                running_loss += loss.item() * inputs.size(0)                   #모든 데이터의 loss합산해서 저장하기 위해 하나의 미니 배치에 대해 계산된 loss값에 데이터수를 곱해 합산함. 이때, inputs.size(0)은 dataloader에서 전달되는 미니배치의 데이터 수를 의미하는 것으로 배치사이즈임. \n",
    "                running_corrects += torch.sum(preds == labels.data)            #15에서 모델을 통해 예측한 값과 target이 같으면 running_corrects를 1만큼 증가시키고, 같지 않으면 증가시키지 않음.\n",
    "            if phase == 'train':                                               #한 epoch당 1번 모델이 현재 학습 단계일 경우에만 실행\n",
    "                scheduler.step()\n",
    "                l_r = [x['lr'] for x in optimizer_ft.param_groups]             #스케쥴러에 의해 learning rate가 조정되는 것을 직접 확인하는 부분\n",
    "                                                                               #optimizer_ft.param_groups의 원소는 학습 과정에서의 parameter를 저장하고 있는 딕셔너리. 이중 learning rate에 해당하는 키인 'lr'이용하여 각 epoch의 learning rate를 불러옴.\n",
    "                print('learning rate: ', l_r)\n",
    "                       \n",
    "            epoch_loss = running_loss/dataset_sizes[phase]                     #epoch의 loss를 계산하기 위해 running_loss를 미리 계산해둔 dataset_size로 나눔.                     \n",
    "            epoch_acc = running_corrects.double()/dataset_sizes[phase]         #해당 epoch의 정확도를 계산하기 위해 running_corrects를 미리 계산해둔 dataset_size로 나눔.\n",
    " \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) #해당 epoch와 현재 모델의 단계, loss값, 정확도 출력\n",
    "    \n",
    "         \n",
    "            if phase == 'val' and epoch_acc > best_acc:                        #검증단계에서 현재 epoch의 정확도가 최고 정확도보다 높다면, best_acc를 현재 epoch의 정확도로 업데이트후\n",
    "                                                                               \n",
    "                best_acc = epoch_acc                                           \n",
    "                best_model_wts = copy.deepcopy(model.state_dict())             #해당 epoch모델을 best_model_wts에 저장\n",
    " \n",
    "        time_elapsed = time.time() - since                                     #한 epoch당 소요된 시간 계산. \n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))                              \n",
    " \n",
    "    model.load_state_dict(best_model_wts)                                      #정확도가 가장 높은 모델을 불러온 후 반환\n",
    "\n",
    "    return model    \n",
    "#학습을 위한 함수 구성 완료."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff613cc-bb04-474b-b476-24bbb9c6f043",
   "metadata": {},
   "source": [
    "# 모델 학습 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da0d44e-5b2d-4506-8378-b61235e7fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet50 = train_resnet(resnet, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH) \n",
    "#앞서 정의한 train_resnet()함수를 이용해 Resnet50모델을 fine-tuning함\n",
    "torch.save(model_resnet50, 'resnet50.pt')\n",
    "#학습이 완룐된 모델인 model_resnet50을 'resnet50.pt'라는 이름의 파일로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f997a-cfee-492e-a8bd-ee2955d5a00a",
   "metadata": {},
   "source": [
    "# 베이스라인 모델 평가를 위한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3a930-b7f3-494e-a3c7-d636b7e8d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_base = transforms.Compose([transforms.Resize([64,64]),transforms.ToTensor()])\n",
    "test_base = ImageFolder(root='./splitted/test',transform=transform_base)  \n",
    "test_loader_base = torch.utils.data.DataLoader(test_base, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "#베이스라인 모델 성능 평가를 위해 사용할 테스트데이터의 dataloader 생성.\n",
    "#모델을 학습시킬 때 사용한 학습 및 검증 데이터와 동일한 방법으로 전처리 수행하고 배치사이즈도 동일하게 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f2875-7da8-4b06-a4c2-588347330b68",
   "metadata": {},
   "source": [
    "# 전이학습 모델 평가를 위한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906cb239-86e8-4337-bb0a-ba0f213bb879",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_resNet = transforms.Compose([\n",
    "        transforms.Resize([64,64]),  \n",
    "        transforms.RandomCrop(52),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "    ])\n",
    "    \n",
    "test_resNet = ImageFolder(root='./splitted/test', transform=transform_resNet) \n",
    "test_loader_resNet = torch.utils.data.DataLoader(test_resNet, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "#전이학슴 모델 성능평가를 위해 사용할 테스트데이터의 dataloader를 생성.\n",
    "#마찬가지로 모델을 학습시킬때 사용한 학습 및 검증 데이터와 동일방법으로 전처리 수행하고 배치사이즈도 동일하게 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9059afc-2392-4bc8-bfcc-17fc0d61e485",
   "metadata": {},
   "source": [
    "# 베이스라인 모델 성능 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72d94a-5788-40bf-99ae-77f2ac3c1b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline=torch.load('baseline.pt')  #저장했던 베이스라인 모델 불러옴\n",
    "baseline.eval()   #모델을 평가모드로 설정\n",
    "test_loss, test_accuracy = evaluate(baseline, test_loader_base) #evaluate함수 이용하여 테스트데이터에 대한 정확도 측정\n",
    "\n",
    "print('baseline test acc:  ', test_accuracy) #평가 정확도 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef8536-48ff-4ca6-b330-27539a9b9e87",
   "metadata": {},
   "source": [
    "# 전이학습 모델 성능 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b237f7d2-3b77-4ef1-9cb3-dce0887dbb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50=torch.load('resnet50.pt')  #저장된 전이학습 모델 불러옴\n",
    "resnet50.eval()  #모델을 평가 모드로 설정\n",
    "test_loss, test_accuracy = evaluate(resnet50, test_loader_resNet) #앞서 정의한 evaluate함수 이용하여 테스트 데이터에 대한 정확도 측정\n",
    "\n",
    "print('ResNet test acc:  ', test_accuracy) #평가 정확도 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96542926-7b5e-4805-971a-351e3ea0ea0d",
   "metadata": {},
   "source": [
    "결과\n",
    "\n",
    "베이스라인 모델보다 파인튜닝한 resnet50모델이 더 높은 정확도 보유. 93% vs 97%\n",
    "\n",
    "=> 모델을 직접 구축하여 처음부터 학습시키는 것보다 많은 양의 데이터셋으로 미리 학습된 모델을 불러와 일부를 fine tuning하는 것이 더 높은 예측 성능을 냄.\n",
    "\n",
    "why?\n",
    "\n",
    "1. 데이터 수와 질.\n",
    "\n",
    "베이스라인 모델 : 40,000 데이터셋 나쁘지 않은 좋은 성능\n",
    "\n",
    "finetuning resnet50 : 약14,000,000 데이터셋 다양한 이미지의 feature가 학습되어있음.(강아지 고양이, 사자, 등)\n",
    "\n",
    "2. 파라미터 시작점\n",
    "\n",
    "베이스라인 모델 : 초기 파라미터 랜덤\n",
    "\n",
    "finetuning resnet50 : 수백만장 이미지 통해 미리 학습한 모델의 파라미터 이용\n",
    "\n",
    "결론 : 전이학습은 이미지, 영상, 자연어 등 여러 분야에 다양하게 사용됌.\n",
    "단순 분류외 물체의 존재 부분 파악하는 localization작업 or 하나의 이미지에 여러 물체 탐지하는 obj detection, 물체 경계를 정확히 파악하는 segmentation기술 등에도 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97b662-ec1a-4d56-9138-2547a6b48aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
